# ğŸ”¤ trigram_lm - a tiny smoothed language-model toolkit

## Problem & Motivation
Characterâ€“ and word-level *n-gram* language models are a classical
baselines for NLP tasks such as scoring text (perplexity) or generating
synthetic sentences. This repo provides a minimal implementation for fast prototyping and educational use.

## ğŸ—ï¸ Architecture / Tech Stack
* ğŸ **Python â‰¥ 3.9** â€” no heavy ML libraries required  
* ğŸ“¦ Pure `collections.Counter` counts â†’ Laplace-smoothed probabilities (more on ngram smoothing: https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html)
* ğŸ“š `src/ngram_model.py` exposes:
  - `CharTrigramModel`
  - `WordTrigramModel`  
  Both inherit from a generic `_BaseNGramModel` (open for extension, closed for bugs)  
* ğŸ’» Command-line UX lives in `src/cli.py` (train / generate / perplexity)

## âš¡ Example: Train â†’ Sample â†’ Evaluate
```bash
python -m src.cli train      --input data/training.en \
                             --output models/char.pkl \
                             --model char --alpha 0.5

python -m src.cli generate   --model models/char.pkl --length 200

python -m src.cli perplexity --model models/char.pkl --input data/heldout.en
```
## âš™ï¸ Environment / Setup
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
pytest  # optional smoke-tests
```
## ğŸ–¥ï¸ CLI Workflow
Hereâ€™s a visual summary of how the command-line interface works.  
You can train a model, generate text, and evaluate its perplexity using three simple subcommands.
```mermaid
graph TD;
    A[User CLI Entry] --> B{Command};
    B --> C[train: read data â†’ train model with alpha smoothing â†’ save .pkl];
    B --> D[generate: load model â†’ output sample text];
    B --> E[perplexity: load model â†’ read file â†’ compute PP score];
```
## ğŸ“‚ Quickstart Data
* ğŸ“˜ Europarl (English subset) â€” used in our example output
  * Download: http://www.statmt.org/europarl/
  * Recommended preprocessing: lowercase, remove XML markup, one sentence per line
* ğŸ“š Project Gutenberg â€” public domain books for stylistic generation
	*	Browse: https://www.gutenberg.org/
	*	Tip: Use plain text UTF-8 downloads, strip headers/footers
*	ğŸ“ Wikitext-2 (Hugging Face) â€” well-formatted Wikipedia-style text
	*	Dataset page: https://huggingface.co/datasets/wikitext
	*	Direct link: https://huggingface.co/datasets/wikitext/viewer/wikitext-2-raw-v1/train
*	ğŸŒ Common Crawl (via OSCAR or CCNet) â€” large-scale web crawl text
	* OSCAR dataset (multilingual, deduplicated Common Crawl): https://oscar-corpus.com/
	* CCNet (preprocessed version of Common Crawl by FAIR): https://github.com/facebookresearch/cc_net

Place training data in the data/ directory (e.g., data/training.en), and optionally split out a validation or held-out set as data/heldout.en.

## âœ¨ Sample Output
```bash
e. the cost of the proposed measure is not in line with our budget.
mr president i believe we should therefore postpone the vote .
```
(Generated by the word-trigram model trained on ~2 MB of Europarl)

## ğŸ“Š Key Results

| ğŸ§© Model  | ğŸ“– Corpus |  ğŸ”§ Î± (Laplace)  | ğŸ“‰ Perplexity â†“ |
| ------------- | ------------- | ------------- | ------------- |
| Char-trigram (ours)  | 1 MB English  | 0.5  | 28.3  |
| Word-trigram (baseline)  | same corpus  | 1.0  | 72,483  |

The character-level trigram model reduced model surprise (perplexity) from ~1600 (word-level) to 28, consistent with values reported in classical NLP literature (~20â€“50).

## â™»ï¸ Re-use
Everything is MIT-licensed.  You can either drop src/ into your own projects, or install from a local clone:
```bash
pip install -e .
```
Happy language-modelling! ğŸ¤–



